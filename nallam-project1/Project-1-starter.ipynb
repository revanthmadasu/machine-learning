{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/revanthmadasu/machine-learning/blob/master/nallam-project1/Project-1-starter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62834763",
      "metadata": {
        "id": "62834763"
      },
      "source": [
        "# Project 1 Starter\n",
        "\n",
        "**This is draft - version 0 - changes are possible and will be anounced.**\n",
        "\n",
        "Project 1 is to allow students to practice Data Science concepts learned so far.\n",
        "\n",
        "The project will include following tasks:\n",
        "- Load dataset. Don't use \"index\" column for training.\n",
        "- Clean up the data:\n",
        "    - Encode replace missing values\n",
        "    - Replace features values that appear incorrect\n",
        "- Encode categorical variables\n",
        "- Split dataset to Train/Validation/Test\n",
        "- Add engineered features\n",
        "- Train and tune ML model\n",
        "- Provide final metrics using Test dataset\n",
        "\n",
        "### Types of models to train\n",
        "\n",
        "Your final submission should include single model. \n",
        "The model set you should try to come up with best model:\n",
        "1. Sklearn Logistic Regression - try all combinations of regularization\n",
        "2. H2O-3 GLM - try different combinations of regularization\n",
        "\n",
        "\n",
        "\n",
        "### Feature engineering\n",
        "\n",
        "You should train/fit categorical features scalers and encoders on Train only. Use `transform` or equivalent function on Validation/Test datasets.\n",
        "\n",
        "It is important to understand all the steps before model training, so that you can reliably replicate and test them to produce scoring function.\n",
        "\n",
        "\n",
        "You should generate various new features. Examples of such features can be seen in the Module-3 lecture on GLMs.\n",
        "Your final model should have at least **10** new engineered features. On-hot-encoding, label encoding, and target encoding is not included in the **10** features.\n",
        "You can try, but target encoding is not expected to produce improvement for Linear models.\n",
        "\n",
        "Ideas for Feature engineering for various types of variables:\n",
        "1. https://docs.h2o.ai/driverless-ai/1-10-lts/docs/userguide/transformations.html\n",
        "2. GLM lecture and hands-on (Module-3)\n",
        "\n",
        "\n",
        "**Note**: \n",
        "- You don't have to perform feature engineering using H2O-3 even if you decided to use H2O-3 GLM for model training.\n",
        "- It is OK to perfor feature engineering using any technique, as long as you can replicate it correctly in the Scoring function.\n",
        "\n",
        "\n",
        "### Threshold calculation\n",
        "\n",
        "You will need to calculate optimal threshold for class assignment using F1 metric:\n",
        "- If using sklearn, use F1 `macro`: `f1_score(y_true, y_pred, average='macro')` \n",
        "- If using H2O-3, use F1\n",
        "\n",
        "You will need to find optimal probability threshold for class assignment, the threshold that maximizes above F1.\n",
        "\n",
        "\n",
        "\n",
        "### Scoring function\n",
        "\n",
        "The Project-1 will be graded based on the completeness and performance of your final model against the hold-out dataset.\n",
        "The hold-out dataset will not be known to the students. As part of your deliverables, you will need to submit a scoring function. The scoring function will perform the following:\n",
        "- Accept dataset in the same format as provided with the project, minus \"MIS_Status\" column\n",
        "- Load trained model and any encoders/scalers that are needed to transform data\n",
        "- Transform dataset into format that can be scored with the trained model\n",
        "- Score the dataset and return the results, for each record\n",
        "    - Record ID\n",
        "    - Record label as determined by final model (0 or 1)\n",
        "    - If your model returns probabilities, you need to assign the label based on maximum F1 threshold\n",
        "    \n",
        "Scoring function header:\n",
        "```\n",
        "def project_1_scoring(data):\n",
        "    \"\"\"\n",
        "    Function to score input dataset.\n",
        "    \n",
        "    Input: dataset in Pandas DataFrame format\n",
        "    Output: Python list of labels in the same order as input records\n",
        "    \n",
        "    Flow:\n",
        "        - Load artifacts\n",
        "        - Transform dataset\n",
        "        - Score dataset\n",
        "        - Return labels\n",
        "    \n",
        "    \"\"\"\n",
        "    l = data.shape[0]\n",
        "    return l*[0]\n",
        "```\n",
        "\n",
        "Look for full example of scoring function at the bottom of the notebook. **Don't copy as is - this is just an example**\n",
        "\n",
        "\n",
        "\n",
        "### Deliverables in a single zip file in the following structure:\n",
        "- `notebook` (folder)\n",
        "    - Jupyter notebook with complete code to manipulate data, train and tune final model. `ipynb` format\n",
        "    - Jupyter notebook in `html` format\n",
        "- `artifacts` (folder)\n",
        "    - Model and any potential encoders in the \"pkl\" format or native H2O-3 format (for H2O-3 model)\n",
        "    - Scoring function that will load the final model and encoders. Separate from above notebook or `.py` file\n",
        "\n",
        "\n",
        "\n",
        "Your notebook should include explanations about your code and be designed to be easily followed and results replicated. Once you are done with the final version, you will need to test it by running all cells from top to bottom after restarting Kernel. It can be done by running `Kernel -> Restart & Run All`\n",
        "\n",
        "\n",
        "**Important**: To speed up progress, first produce working code using a small subset of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "dbeaf5cb",
      "metadata": {
        "id": "dbeaf5cb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef8e26a1-8453-490c-942f-2789a585ead1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.9/dist-packages (from scikit-learn) (1.22.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn) (1.1.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.9/dist-packages (from scikit-learn) (1.10.1)\n"
          ]
        }
      ],
      "source": [
        "pip install scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install category-encoders"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oRpMqjRokZqp",
        "outputId": "504c4d4d-99e5-4396-d1a0-3dd0af6117dc"
      },
      "id": "oRpMqjRokZqp",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: category-encoders in /usr/local/lib/python3.9/dist-packages (2.6.0)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.9/dist-packages (from category-encoders) (1.22.4)\n",
            "Requirement already satisfied: pandas>=1.0.5 in /usr/local/lib/python3.9/dist-packages (from category-encoders) (1.4.4)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from category-encoders) (1.10.1)\n",
            "Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.9/dist-packages (from category-encoders) (0.5.3)\n",
            "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.9/dist-packages (from category-encoders) (0.13.5)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.9/dist-packages (from category-encoders) (1.2.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.0.5->category-encoders) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.0.5->category-encoders) (2.8.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from patsy>=0.5.1->category-encoders) (1.16.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.20.0->category-encoders) (3.1.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.20.0->category-encoders) (1.1.1)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.9/dist-packages (from statsmodels>=0.9.0->category-encoders) (23.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c341cb74",
      "metadata": {
        "id": "c341cb74"
      },
      "source": [
        "## Dataset description\n",
        "\n",
        "The dataset is from the U.S. Small Business Administration (SBA) The U.S. SBA was founded in 1953 on the principle of promoting and assisting small enterprises in the U.S. credit market (SBA Overview and History, US Small Business Administration (2015)). Small businesses have been a primary source of job creation in the United States; therefore, fostering small business formation and growth has social benefits by creating job opportunities and reducing unemployment. There have been many success stories of start-ups receiving SBA loan guarantees such as FedEx and Apple Computer. However, there have also been stories of small businesses and/or start-ups that have defaulted on their SBA-guaranteed loans.  \n",
        "More info on the original dataset: https://www.kaggle.com/mirbektoktogaraev/should-this-loan-be-approved-or-denied\n",
        "\n",
        "**Don't use original dataset, use only dataset provided with project requirements in eLearning**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0c9b618",
      "metadata": {
        "id": "a0c9b618"
      },
      "source": [
        "## Preparation\n",
        "\n",
        "Use dataset provided in the eLearning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "3ca7e035",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "3ca7e035",
        "outputId": "7ac25c7a-5d94-42a8-a7a0-85357381d7cf"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>.container { width:80% !important; }</style>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import pandas as pd\n",
        "pd.set_option('display.max_columns', 1500)\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "#Extend cell width\n",
        "from IPython.core.display import display, HTML\n",
        "display(HTML(\"<style>.container { width:80% !important; }</style>\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb889838",
      "metadata": {
        "id": "eb889838"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Created on Mon Mar 18 18:25:50 2019\n",
        "\n",
        "@author: Uri Smashnov\n",
        "\n",
        "Purpose: Analyze input Pandas DataFrame and return stats per column\n",
        "Details: The function calculates levels for categorical variables and allows to analyze summarized information\n",
        "\n",
        "To view wide table set following Pandas options:\n",
        "pd.set_option('display.width', 1000)\n",
        "pd.set_option('max_colwidth',200)\n",
        "\"\"\"\n",
        "import pandas as pd\n",
        "def describe_more(df,normalize_ind=False, weight_column=None, skip_columns=[], dropna=True):\n",
        "    var = [] ; l = [] ; t = []; unq =[]; min_l = []; max_l = [];\n",
        "    assert isinstance(skip_columns, list), \"Argument skip_columns should be list\"\n",
        "    if weight_column is not None:\n",
        "        if weight_column not in list(df.columns):\n",
        "            raise AssertionError('weight_column is not a valid column name in the input DataFrame')\n",
        "      \n",
        "    for x in df:\n",
        "        if x in skip_columns:\n",
        "            pass\n",
        "        else:\n",
        "            var.append( x )\n",
        "            uniq_counts = len(pd.value_counts(df[x],dropna=dropna))\n",
        "            uniq_counts = len(pd.value_counts(df[x], dropna=dropna)[pd.value_counts(df[x],dropna=dropna)>0])\n",
        "            l.append(uniq_counts)\n",
        "            t.append( df[ x ].dtypes )\n",
        "            min_l.append(df[x].apply(str).str.len().min())\n",
        "            max_l.append(df[x].apply(str).str.len().max())\n",
        "            if weight_column is not None and x not in skip_columns:\n",
        "                df2 = df.groupby(x).agg({weight_column: 'sum'}).sort_values(weight_column, ascending=False)\n",
        "                df2['authtrans_vts_cnt']=((df2[weight_column])/df2[weight_column].sum()).round(2)\n",
        "                unq.append(df2.head(n=100).to_dict()[weight_column])\n",
        "            else:\n",
        "                df_cat_d = df[x].value_counts(normalize=normalize_ind,dropna=dropna).round(decimals=2)\n",
        "                df_cat_d = df_cat_d[df_cat_d>0]\n",
        "                #unq.append(df[x].value_counts().iloc[0:100].to_dict())\n",
        "                unq.append(df_cat_d.iloc[0:100].to_dict())\n",
        "            \n",
        "    levels = pd.DataFrame( { 'A_Variable' : var , 'Levels' : l , 'Datatype' : t ,\n",
        "                             'Min Length' : min_l,\n",
        "                             'Max Length': max_l,\n",
        "                             'Level_Values' : unq} )\n",
        "    #levels.sort_values( by = 'Levels' , inplace = True )\n",
        "    return levels"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cee2c254",
      "metadata": {
        "id": "cee2c254"
      },
      "source": [
        "### Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "a19bd163",
      "metadata": {
        "id": "a19bd163"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('./data/SBA_loans_project_1.zip')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2301a5ed",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2301a5ed",
        "outputId": "ecef4c98-f7c9-4317-8a06-eeff7399d838"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data shape: (809247, 21)\n"
          ]
        }
      ],
      "source": [
        "print(\"Data shape:\", data.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab8b4aea",
      "metadata": {
        "id": "ab8b4aea"
      },
      "source": [
        "**Review dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "547f9eb1",
      "metadata": {
        "id": "547f9eb1"
      },
      "outputs": [],
      "source": [
        "desc_df = describe_more(data)\n",
        "desc_df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5fe1af41",
      "metadata": {
        "id": "5fe1af41"
      },
      "source": [
        "## Dataset preparation and clean-up\n",
        "\n",
        "Modify and clean-up the dataset as following:\n",
        "- Replace encode Na/Null values\n",
        "- Convert the strings styled as '$XXXX.XX' to float values. Columns = ['DisbursementGross', 'BalanceGross', 'GrAppv', 'SBA_Appv']\n",
        "- Convert MIS_Status to 0/1. Make value \"CHGOFF\" as 1\n",
        "\n",
        "Any additional clean-up as you find fit."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Removing Duplicates"
      ],
      "metadata": {
        "id": "bQ0psusDKqrk"
      },
      "id": "bQ0psusDKqrk"
    },
    {
      "cell_type": "code",
      "source": [
        "# removing duplicates\n",
        "data = data.drop_duplicates()\n",
        "print(\"Data shape:\", data.shape)"
      ],
      "metadata": {
        "id": "ZaRa4KNm0iEy",
        "outputId": "5795d5bf-f6f2-4e9d-cbdc-00024bd7f974",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "ZaRa4KNm0iEy",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data shape: (809247, 21)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Removing index and converting currency to float values"
      ],
      "metadata": {
        "id": "yI9bAVjKKxhW"
      },
      "id": "yI9bAVjKKxhW"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "0f9f9eaa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0f9f9eaa",
        "outputId": "08f943d7-6f5b-453e-bae6-259253b45310"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "City                                  GLEN BURNIE\n",
              "State                                          MD\n",
              "Zip                                         21060\n",
              "Bank                 BUSINESS FINANCE GROUP, INC.\n",
              "BankState                                      VA\n",
              "NAICS                                      811111\n",
              "Term                                          240\n",
              "NoEmp                                           7\n",
              "NewExist                                      1.0\n",
              "CreateJob                                       6\n",
              "RetainedJob                                     7\n",
              "FranchiseCode                                   1\n",
              "UrbanRural                                      1\n",
              "RevLineCr                                       0\n",
              "LowDoc                                          N\n",
              "DisbursementGross                        743000.0\n",
              "BalanceGross                                  0.0\n",
              "GrAppv                                   743000.0\n",
              "SBA_Appv                                 743000.0\n",
              "MIS_Status                                  P I F\n",
              "Name: 0, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "# remove index\n",
        "processed_data = data.loc[:,data.columns!='index']\n",
        "def dollar_string_to_float(dollar_str):\n",
        "  dollar_str = dollar_str.replace(',', '')\n",
        "  dollar_float = float(dollar_str[1:])\n",
        "  return dollar_float\n",
        "currency_cols = ['DisbursementGross', 'BalanceGross', 'GrAppv', 'SBA_Appv']\n",
        "for colname in currency_cols:\n",
        "  processed_data[colname] = processed_data[colname].apply(dollar_string_to_float)\n",
        "processed_data.loc[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convert MIS_Status to 0/1. Make value \"CHGOFF\" as 1"
      ],
      "metadata": {
        "id": "hcRopxc8K5YF"
      },
      "id": "hcRopxc8K5YF"
    },
    {
      "cell_type": "code",
      "source": [
        "processed_data['MIS_Status'] = processed_data['MIS_Status'].replace('P I F', 0)\n",
        "processed_data['MIS_Status'] = processed_data['MIS_Status'].replace('CHGOFF', 1)\n",
        "processed_data = processed_data.dropna()\n",
        "processed_data['MIS_Status'].unique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJlBnwrPvIg4",
        "outputId": "c7450813-1528-469e-9e95-267d33bd0665"
      },
      "id": "WJlBnwrPvIg4",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 1.])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "processed_data.info()"
      ],
      "metadata": {
        "id": "PK_OOX3d7U3W",
        "outputId": "80716ab0-92ad-4401-b02f-eb852ef5fb42",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "PK_OOX3d7U3W",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 784130 entries, 0 to 809246\n",
            "Data columns (total 20 columns):\n",
            " #   Column             Non-Null Count   Dtype   \n",
            "---  ------             --------------   -----   \n",
            " 0   City               784130 non-null  category\n",
            " 1   State              784130 non-null  category\n",
            " 2   Zip                784130 non-null  int64   \n",
            " 3   Bank               784130 non-null  category\n",
            " 4   BankState          784130 non-null  category\n",
            " 5   NAICS              784130 non-null  int64   \n",
            " 6   Term               784130 non-null  int64   \n",
            " 7   NoEmp              784130 non-null  int64   \n",
            " 8   NewExist           784130 non-null  float64 \n",
            " 9   CreateJob          784130 non-null  int64   \n",
            " 10  RetainedJob        784130 non-null  int64   \n",
            " 11  FranchiseCode      784130 non-null  int64   \n",
            " 12  UrbanRural         784130 non-null  int64   \n",
            " 13  RevLineCr          784130 non-null  category\n",
            " 14  LowDoc             784130 non-null  category\n",
            " 15  DisbursementGross  784130 non-null  float64 \n",
            " 16  BalanceGross       784130 non-null  float64 \n",
            " 17  GrAppv             784130 non-null  float64 \n",
            " 18  SBA_Appv           784130 non-null  float64 \n",
            " 19  MIS_Status         784130 non-null  float64 \n",
            "dtypes: category(6), float64(6), int64(8)\n",
            "memory usage: 97.1 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Handling null data for individual columns.\n",
        "As we have already removed rows with null data this does not have any impact. \n",
        "Since we are having large amounts of data, removing rows with null data would not result in data loss. Thats why we have choose to remove rows with null data\n",
        "To handle null data instead of removing, comment `processed_data = processed_data.dropna()` and run this code\n"
      ],
      "metadata": {
        "id": "76UBRPJdLNPC"
      },
      "id": "76UBRPJdLNPC"
    },
    {
      "cell_type": "code",
      "source": [
        "# handling null data\n",
        "row_null_counts = processed_data.isna().any(axis=1).sum()\n",
        "print(\"\\nNumber of rows with null values:\", row_null_counts)\n",
        "def check_null_counts():\n",
        "  col_null_counts = dict()\n",
        "  for column in processed_data.columns:\n",
        "    col_null_count = processed_data[column].isna().sum()\n",
        "    if col_null_count:\n",
        "      col_null_counts[column] = col_null_count\n",
        "  return col_null_counts\n",
        "print(f'before handling: {check_null_counts()}')\n",
        "categoral_null_cols = ['City', 'State', 'Bank', 'BankState', 'RevLineCr', 'LowDoc']\n",
        "for null_cat_col in categoral_null_cols:\n",
        "  processed_data[null_cat_col] = processed_data[null_cat_col].fillna('unknown').astype('category')\n",
        "processed_data['NewExist'] = processed_data['NewExist'].fillna(0)\n",
        "print(f'after handling handling: {check_null_counts()}')"
      ],
      "metadata": {
        "id": "WnHdYBCm1gt9",
        "outputId": "8f842576-dc02-45ad-e8ae-11902fda5e65",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "WnHdYBCm1gt9",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of rows with null values: 0\n",
            "before handling: {}\n",
            "after handling handling: {}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "processed_data['MIS_Status'].value_counts()"
      ],
      "metadata": {
        "id": "4WzV6vja7o80",
        "outputId": "4c76d4af-2283-4b95-f008-3867889e5ff0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "4WzV6vja7o80",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0    658734\n",
              "1.0    140776\n",
              "Name: MIS_Status, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "processed_data['RevLineCr'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wq5z2iL9J_aN",
        "outputId": "1a409ce1-c9e1-409b-f0cb-54af16a3006d"
      },
      "id": "Wq5z2iL9J_aN",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "N    374421\n",
              "0    231797\n",
              "Y    179628\n",
              "T     13606\n",
              "1        22\n",
              "R        12\n",
              "`         9\n",
              "2         6\n",
              "C         2\n",
              "-         1\n",
              ".         1\n",
              "3         1\n",
              "5         1\n",
              "7         1\n",
              "A         1\n",
              "Q         1\n",
              "Name: RevLineCr, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Engineering: \n",
        "\n",
        "Processing RevLineCr feature. Removing rows with unrecognised categories and replacing with binary categories"
      ],
      "metadata": {
        "id": "kvaHT5WMMbpf"
      },
      "id": "kvaHT5WMMbpf"
    },
    {
      "cell_type": "code",
      "source": [
        "processed_data['RevLineCr'] = processed_data['RevLineCr'].replace('N', 0)\n",
        "processed_data['RevLineCr'] = processed_data['RevLineCr'].replace('0', 0)\n",
        "processed_data['RevLineCr'] = processed_data['RevLineCr'].replace('Y', 1)\n",
        "for remove_cat in processed_data['RevLineCr'].cat.categories.tolist():\n",
        "  if remove_cat not in [0,1]:\n",
        "    processed_data = processed_data[processed_data['RevLineCr'] != remove_cat]\n",
        "    processed_data['RevLineCr'] = processed_data['RevLineCr'].cat.remove_categories(remove_cat)\n",
        "\n",
        "processed_data['RevLineCr'].value_counts()"
      ],
      "metadata": {
        "id": "0VN0Jlns7nbI",
        "outputId": "57171b0a-d99c-4d49-9506-4b177902ad6e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "0VN0Jlns7nbI",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    606218\n",
              "1    179628\n",
              "Name: RevLineCr, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "processed_data['MIS_Status'].unique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BybhXgPUpcls",
        "outputId": "6566bac8-7a61-4a80-96af-e72d7d03b4b5"
      },
      "id": "BybhXgPUpcls",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 1.])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Engineering:\n",
        "Processing NewExist feature. Replacing with binary categories"
      ],
      "metadata": {
        "id": "69YAyzYuMtcg"
      },
      "id": "69YAyzYuMtcg"
    },
    {
      "cell_type": "code",
      "source": [
        "processed_data['NewExist'] = processed_data['NewExist'].replace(1, 0)\n",
        "processed_data['NewExist'] = processed_data['NewExist'].replace(2, 1)"
      ],
      "metadata": {
        "id": "S5LkmHUXIHgz"
      },
      "id": "S5LkmHUXIHgz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Engineering:\n",
        "\n",
        "Processing LowDoc feature. Removing rows with unrecognised categories and replacing with binary categories"
      ],
      "metadata": {
        "id": "g1MwFKQXM3vI"
      },
      "id": "g1MwFKQXM3vI"
    },
    {
      "cell_type": "code",
      "source": [
        "processed_data['LowDoc'] = processed_data['LowDoc'].replace('Y', 1)\n",
        "processed_data['LowDoc'] = processed_data['LowDoc'].replace('N', 0)\n",
        "processed_data['LowDoc'] = processed_data['LowDoc'].replace('0', 0)\n",
        "processed_data['LowDoc'] = processed_data['LowDoc'].replace('1', 1)\n",
        "for remove_cat in processed_data['LowDoc'].cat.categories.tolist():\n",
        "  if remove_cat not in [0,1]:\n",
        "    processed_data = processed_data[processed_data['LowDoc'] != remove_cat]\n",
        "    processed_data['LowDoc'] = processed_data['LowDoc'].cat.remove_categories(remove_cat)\n",
        "processed_data['LowDoc'].value_counts()\n",
        "# processed_data['LowDoc'].cat.categories"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LW0fau1kTVlF",
        "outputId": "09fa2298-92f1-4405-8936-6d377ee7548c"
      },
      "id": "LW0fau1kTVlF",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    686998\n",
              "1     97132\n",
              "Name: LowDoc, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "processed_data['UrbanRural'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1gtJRjhxqJkG",
        "outputId": "c32b709d-50be-4fd4-9e01-722aeebc4553"
      },
      "id": "1gtJRjhxqJkG",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    408049\n",
              "0    284083\n",
              "2     91998\n",
              "Name: UrbanRural, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7f21dcf",
      "metadata": {
        "id": "b7f21dcf"
      },
      "source": [
        "## Categorical and numerical variables encoding\n",
        "\n",
        "Encode categorical variables using either one of the techniques below. Don't use LabelEncoder.\n",
        "- One-hot-encoder for variables with less than 10 valid values. Name your new columns \"Original_name\"_valid_value\n",
        "- Target encoder from the following library: https://contrib.scikit-learn.org/category_encoders/index.html . Name your new column \"Original_name\"_trg\n",
        "- WOE encoder from the following library: https://contrib.scikit-learn.org/category_encoders/index.html . Name your new column \"Original_name\"_woe\n",
        "\n",
        "\n",
        "WOE encoder can be used with numerical variables too. \n",
        "\n",
        "\n",
        "Example of use for target encoder:\n",
        "```\n",
        "import category_encoders as ce\n",
        "\n",
        "encoder = ce.TargetEncoder(cols=[...])\n",
        "\n",
        "encoder.fit(X, y)\n",
        "X_cleaned = encoder.transform(X_dirty)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sampling to increase run time"
      ],
      "metadata": {
        "id": "-RrtdEexNPCi"
      },
      "id": "-RrtdEexNPCi"
    },
    {
      "cell_type": "code",
      "source": [
        "s_processed_data = processed_data.sample(frac=0.0125, random_state=27)\n",
        "s_processed_data\n",
        "print(s_processed_data.shape)"
      ],
      "metadata": {
        "id": "2ZiIsG0dMK6Y",
        "outputId": "609d92e4-04eb-4f17-c0cc-2d25816d538a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "2ZiIsG0dMK6Y",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(9802, 20)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cat_cols_bin_en = ['City', 'State', 'Bank', 'BankState', 'Zip', 'NAICS', 'UrbanRural']"
      ],
      "metadata": {
        "id": "B7vmNtvgWHvY"
      },
      "id": "B7vmNtvgWHvY",
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s_processed_data.info()"
      ],
      "metadata": {
        "id": "e1daKaVu-nIv",
        "outputId": "31d326e9-74d6-434a-a880-a0b096211065",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "e1daKaVu-nIv",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 3921 entries, 724290 to 635320\n",
            "Data columns (total 20 columns):\n",
            " #   Column             Non-Null Count  Dtype   \n",
            "---  ------             --------------  -----   \n",
            " 0   City               3921 non-null   category\n",
            " 1   State              3921 non-null   category\n",
            " 2   Zip                3921 non-null   int64   \n",
            " 3   Bank               3921 non-null   category\n",
            " 4   BankState          3921 non-null   category\n",
            " 5   NAICS              3921 non-null   int64   \n",
            " 6   Term               3921 non-null   int64   \n",
            " 7   NoEmp              3921 non-null   int64   \n",
            " 8   NewExist           3921 non-null   float64 \n",
            " 9   CreateJob          3921 non-null   int64   \n",
            " 10  RetainedJob        3921 non-null   int64   \n",
            " 11  FranchiseCode      3921 non-null   int64   \n",
            " 12  UrbanRural         3921 non-null   int64   \n",
            " 13  RevLineCr          3921 non-null   category\n",
            " 14  LowDoc             3921 non-null   category\n",
            " 15  DisbursementGross  3921 non-null   float64 \n",
            " 16  BalanceGross       3921 non-null   float64 \n",
            " 17  GrAppv             3921 non-null   float64 \n",
            " 18  SBA_Appv           3921 non-null   float64 \n",
            " 19  MIS_Status         3921 non-null   float64 \n",
            "dtypes: category(6), float64(6), int64(8)\n",
            "memory usage: 1.9 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Choosing Binary Encoder\n",
        "As there are many categorical variables, one-hot encoding can result in high dimensional sparse feature space."
      ],
      "metadata": {
        "id": "RbKCOOSI-IxK"
      },
      "id": "RbKCOOSI-IxK"
    },
    {
      "cell_type": "code",
      "source": [
        "from category_encoders import BinaryEncoder\n",
        "import pandas as pd\n",
        "bin_encoder = BinaryEncoder(cols=cat_cols_bin_en)\n",
        "bin_encoded_data = bin_encoder.fit_transform(processed_data)"
      ],
      "metadata": {
        "id": "mNqOdrJRkIlK"
      },
      "id": "mNqOdrJRkIlK",
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bin_encoded_data.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3oqPOXOgm8uC",
        "outputId": "8fda291b-43ee-49d8-8c24-fd952368002d"
      },
      "id": "3oqPOXOgm8uC",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['City_0', 'City_1', 'City_2', 'City_3', 'City_4', 'City_5', 'City_6',\n",
              "       'City_7', 'City_8', 'City_9', 'City_10', 'City_11', 'City_12',\n",
              "       'City_13', 'City_14', 'State_0', 'State_1', 'State_2', 'State_3',\n",
              "       'State_4', 'State_5', 'Zip_0', 'Zip_1', 'Zip_2', 'Zip_3', 'Zip_4',\n",
              "       'Zip_5', 'Zip_6', 'Zip_7', 'Zip_8', 'Zip_9', 'Zip_10', 'Zip_11',\n",
              "       'Zip_12', 'Zip_13', 'Zip_14', 'Bank_0', 'Bank_1', 'Bank_2', 'Bank_3',\n",
              "       'Bank_4', 'Bank_5', 'Bank_6', 'Bank_7', 'Bank_8', 'Bank_9', 'Bank_10',\n",
              "       'Bank_11', 'Bank_12', 'BankState_0', 'BankState_1', 'BankState_2',\n",
              "       'BankState_3', 'BankState_4', 'BankState_5', 'NAICS_0', 'NAICS_1',\n",
              "       'NAICS_2', 'NAICS_3', 'NAICS_4', 'NAICS_5', 'NAICS_6', 'NAICS_7',\n",
              "       'NAICS_8', 'NAICS_9', 'NAICS_10', 'Term', 'NoEmp', 'NewExist',\n",
              "       'CreateJob', 'RetainedJob', 'FranchiseCode', 'UrbanRural_0',\n",
              "       'UrbanRural_1', 'RevLineCr', 'LowDoc', 'DisbursementGross',\n",
              "       'BalanceGross', 'GrAppv', 'SBA_Appv', 'MIS_Status'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_data = bin_encoded_data"
      ],
      "metadata": {
        "id": "TGk8qTn1k-zg"
      },
      "id": "TGk8qTn1k-zg",
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_data.iloc[:, 66:-1].columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZT7P4n7dJ_l",
        "outputId": "331cba99-78a4-4e5b-ea35-cd3e8daf5a93"
      },
      "id": "vZT7P4n7dJ_l",
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Term', 'NoEmp', 'NewExist', 'CreateJob', 'RetainedJob',\n",
              "       'FranchiseCode', 'UrbanRural_0', 'UrbanRural_1', 'RevLineCr', 'LowDoc',\n",
              "       'DisbursementGross', 'BalanceGross', 'GrAppv', 'SBA_Appv'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test Train splitting"
      ],
      "metadata": {
        "id": "aE-rpTkVNkwH"
      },
      "id": "aE-rpTkVNkwH"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X = encoded_data.iloc[:, :-1].values\n",
        "y = encoded_data.iloc[:, -1].values\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=44)"
      ],
      "metadata": {
        "id": "DwREpI5SEtJw"
      },
      "id": "DwREpI5SEtJw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "7bb0bfa4",
      "metadata": {
        "id": "7bb0bfa4"
      },
      "source": [
        "# Model Training\n",
        "\n",
        "See Project summary for types of models"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using Logistic Regression\n",
        "Selecting Logistic Regression because it is a Binary Classification problem on large dataset. Logistic Regression is specifically designed for binary classification problems.\n",
        "\n",
        "As it a loan approval problem, the relationship between input and output variables must be mostly linear. Logistic Regression algorithms would be good fit for Linear data.\n",
        "\n"
      ],
      "metadata": {
        "id": "_8K72bi78bla"
      },
      "id": "_8K72bi78bla"
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "id": "265acf5d",
      "metadata": {
        "id": "265acf5d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad2922d4-0cdc-4374-f83c-d549a8c4448d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8293182678042331\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "# evaluate the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ced690c6",
      "metadata": {
        "id": "ced690c6"
      },
      "source": [
        "## Model Tuning\n",
        "\n",
        "Choose one model from the above list. You should provide reasoning on why you have picked the model over others. Perform tuning for the selected model:\n",
        "- Hyper-parameter tuning. Your hyper-parameter search space should have at least 50 combinations.\n",
        "- To avoid overfitting and provide you with reasonable estimate of model performance on hold-out dataset, you will need to split your dataset as following:\n",
        "    - Train, will be used to train model\n",
        "    - Validation, will be used to validate model each round of training\n",
        "    - Testing, will be used to provide final performance metrics, used only once on the final model\n",
        "- Feature engineering. See project description\n",
        "\n",
        "**Selelct final model that produces best performance on the Test dataset.**\n",
        "- For the best model, calculate probability threshold to maximize F1. "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Grid search for hyper parameter tuning"
      ],
      "metadata": {
        "id": "4zf9ee2kNwUx"
      },
      "id": "4zf9ee2kNwUx"
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "id": "674ab5b8",
      "metadata": {
        "id": "674ab5b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13c1fe21-fd11-46fa-f1bb-089d97fdb455"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 2 folds for each of 54 candidates, totalling 108 fits\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "logreg = LogisticRegression()\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.3)\n",
        "# Define the hyperparameter search space\n",
        "param_grid = {\n",
        "    'penalty': ['l2', 'elasticnet', 'none'],\n",
        "    'C': [0.01, 0.1, 1.0],\n",
        "    'fit_intercept': [True, False],\n",
        "    'solver': ['newton-cg', 'lbfgs', 'saga'],\n",
        "    'max_iter': [100]\n",
        "}\n",
        "\n",
        "# Define the GridSearchCV object\n",
        "grid_search = GridSearchCV(logreg, param_grid, cv=2, n_jobs=-1, verbose=1, \n",
        "                           scoring='accuracy')\n",
        "\n",
        "# Fit the GridSearchCV object to the training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "val_score = grid_search.score(X_val, y_val)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Best hyperparameters: {grid_search.best_params_}\")\n",
        "print(f\"Validation score: {val_score}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mGDpXEJeefbS",
        "outputId": "b37aa60c-ec53-4eb0-96cf-9f1081a1128e"
      },
      "id": "mGDpXEJeefbS",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best hyperparameters: {'C': 1.0, 'fit_intercept': False, 'max_iter': 100, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
            "Validation score: 0.850360060189166\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = grid_search.best_estimator_\n",
        "score = best_model.score(X_test, y_test)\n",
        "\n",
        "print(\"Best model score:\", score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1OoK_cWJfdHt",
        "outputId": "283c3f14-ff8c-4a06-bd26-d31dbd071bae"
      },
      "id": "1OoK_cWJfdHt",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model score: 0.8517082626605282\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30bc381c",
      "metadata": {
        "id": "30bc381c"
      },
      "source": [
        "## Save all artifacts\n",
        "\n",
        "Save all artifacts needed for scoring function:\n",
        "- Trained model\n",
        "- Encoders\n",
        "- Any other arficats you will need for scoring\n",
        "\n",
        "**You should stop your notebook here. Scoring function should be in a separate file/notebook.**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install category_encoders"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jHQp6nZ4RnpZ",
        "outputId": "958d848a-fee2-4220-f5cb-f0eb703e8fc0"
      },
      "id": "jHQp6nZ4RnpZ",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting category_encoders\n",
            "  Downloading category_encoders-2.6.0-py2.py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.2/81.2 KB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.9/dist-packages (from category_encoders) (0.13.5)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from category_encoders) (1.10.1)\n",
            "Requirement already satisfied: pandas>=1.0.5 in /usr/local/lib/python3.9/dist-packages (from category_encoders) (1.4.4)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.9/dist-packages (from category_encoders) (1.22.4)\n",
            "Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.9/dist-packages (from category_encoders) (0.5.3)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.9/dist-packages (from category_encoders) (1.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.0.5->category_encoders) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.0.5->category_encoders) (2022.7.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from patsy>=0.5.1->category_encoders) (1.16.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.20.0->category_encoders) (1.1.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.20.0->category_encoders) (3.1.0)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.9/dist-packages (from statsmodels>=0.9.0->category_encoders) (23.0)\n",
            "Installing collected packages: category_encoders\n",
            "Successfully installed category_encoders-2.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Artifact: ModelFormation\n",
        "ModelFormation class does all the preprocessing, model training, test train splitting.\n",
        "It has the best Logistic Regression model with hyper tuning.\n",
        "\n",
        "trainedModel object will be stored in artifact map, and this is used in model scoring function. \n",
        "\n",
        "As this is a class, in order to for python to have a definition to create trainedModel instance, it needs class definition. \n",
        "\n",
        "The same definition will be provided in model scoring function\n",
        "\n",
        "Note that in model scoring function we are not actually training data, the class definition is just there for python to create and instance of object from artifact"
      ],
      "metadata": {
        "id": "swMa40skI77T"
      },
      "id": "swMa40skI77T"
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "3cc128b9",
      "metadata": {
        "id": "3cc128b9"
      },
      "outputs": [],
      "source": [
        "from category_encoders import BinaryEncoder\n",
        "import pandas as pd\n",
        "class ModelFormation:\n",
        "    def process_encode_data(self, data):\n",
        "        processed_data = data.loc[:,data.columns!='index']\n",
        "\n",
        "        def dollar_string_to_float(dollar_str):\n",
        "          dollar_str = dollar_str.replace(',', '')\n",
        "          dollar_float = float(dollar_str[1:])\n",
        "          return dollar_float\n",
        "        # converting dollar to float\n",
        "        currency_cols = ['DisbursementGross', 'BalanceGross', 'GrAppv', 'SBA_Appv']\n",
        "        for colname in currency_cols:\n",
        "          processed_data[colname] = processed_data[colname].apply(dollar_string_to_float)\n",
        "\n",
        "        processed_data = processed_data.dropna()\n",
        "        \n",
        "        processed_data['MIS_Status'] = processed_data['MIS_Status'].replace('P I F', 0)\n",
        "        processed_data['MIS_Status'] = processed_data['MIS_Status'].replace('CHGOFF', 1)\n",
        "\n",
        "        processed_data['RevLineCr'] = processed_data['RevLineCr'].astype('category')\n",
        "\n",
        "        processed_data['RevLineCr'] = processed_data['RevLineCr'].replace('N', 0)\n",
        "        processed_data['RevLineCr'] = processed_data['RevLineCr'].replace('0', 0)\n",
        "        processed_data['RevLineCr'] = processed_data['RevLineCr'].replace('Y', 1)\n",
        "        for remove_cat in processed_data['RevLineCr'].cat.categories.tolist():\n",
        "          if remove_cat not in [0,1]:\n",
        "            processed_data = processed_data[processed_data['RevLineCr'] != remove_cat]\n",
        "            processed_data['RevLineCr'] = processed_data['RevLineCr'].cat.remove_categories(remove_cat)\n",
        "\n",
        "        processed_data['NewExist'] = processed_data['NewExist'].replace(1, 0)\n",
        "        processed_data['NewExist'] = processed_data['NewExist'].replace(2, 1)\n",
        "        \n",
        "        processed_data['LowDoc'] = processed_data['LowDoc'].astype('category')\n",
        "        processed_data['LowDoc'] = processed_data['LowDoc'].replace('Y', 1)\n",
        "        processed_data['LowDoc'] = processed_data['LowDoc'].replace('N', 0)\n",
        "        processed_data['LowDoc'] = processed_data['LowDoc'].replace('0', 0)\n",
        "        processed_data['LowDoc'] = processed_data['LowDoc'].replace('1', 1)\n",
        "        for remove_cat in processed_data['LowDoc'].cat.categories.tolist():\n",
        "          if remove_cat not in [0,1]:\n",
        "            processed_data = processed_data[processed_data['LowDoc'] != remove_cat]\n",
        "            processed_data['LowDoc'] = processed_data['LowDoc'].cat.remove_categories(remove_cat)\n",
        "        processed_data['LowDoc'].value_counts()\n",
        "\n",
        "        s_processed_data = processed_data.sample(frac=0.5, random_state=27)\n",
        "\n",
        "        cat_cols_bin_en = ['City', 'State', 'Bank', 'BankState', 'Zip', 'NAICS', 'UrbanRural']\n",
        "\n",
        "        from category_encoders import BinaryEncoder\n",
        "        import pandas as pd\n",
        "        bin_encoder = BinaryEncoder(cols=cat_cols_bin_en)\n",
        "        bin_encoded_data = bin_encoder.fit_transform(processed_data)\n",
        "\n",
        "        return bin_encoded_data\n",
        "\n",
        "    def test_train_split(self, encoded_data):\n",
        "        from sklearn.model_selection import train_test_split\n",
        "\n",
        "        X = encoded_data.iloc[:, :-1].values\n",
        "        y = encoded_data.iloc[:, -1].values\n",
        "\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=44)\n",
        "\n",
        "        self.x_cols_to_score = encoded_data.iloc[:, :-1].columns\n",
        "\n",
        "        return X_train, X_test, y_train, y_test\n",
        "    def get_cols_to_score(self, start=66, end=-1):\n",
        "      return self.x_cols_to_score[start:end]\n",
        "\n",
        "    def get_model(self, data):\n",
        "      encoded_data = self.process_encode_data(data)\n",
        "      X_train, X_test, y_train, y_test = self.test_train_split(encoded_data)\n",
        "      from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "      lr_model = LogisticRegression(C= 1.0, fit_intercept= False, max_iter= 100, penalty= 'l2', solver='newton-cg')\n",
        "      lr_model.fit(X_train, y_train)\n",
        "\n",
        "      return lr_model\n",
        "\n",
        "\n",
        "    def __init__(self, train_model = False, data = None):\n",
        "      # remove index\n",
        "      if train_model:\n",
        "        self.trained_model = self.get_model(data)\n",
        "\n",
        "\n",
        "trainedModel = ModelFormation(True, data)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainedModel.trained_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "O-1GnkJ-WEw2",
        "outputId": "3b7cb2d4-187d-41f4-faae-75f9a9a0f690"
      },
      "id": "O-1GnkJ-WEw2",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(fit_intercept=False, solver='newton-cg')"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(fit_intercept=False, solver=&#x27;newton-cg&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(fit_intercept=False, solver=&#x27;newton-cg&#x27;)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculating Threshold\n"
      ],
      "metadata": {
        "id": "VViQcj0w68nx"
      },
      "id": "VViQcj0w68nx"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "import numpy as np\n",
        "X_train, X_test, y_train, y_test = trainedModel.test_train_split(encoded_data)\n",
        "y_pred = trainedModel.trained_model.predict(X_test)\n",
        "thresholds = np.arange(0.1, 1.0, 0.1)\n",
        "\n",
        "# Calculate the F1 score for each threshold\n",
        "f1_scores = [f1_score(y_test, y_pred >= t) for t in thresholds]\n",
        "\n",
        "# Find the threshold that maximizes the F1 score\n",
        "best_threshold = thresholds[np.argmax(f1_scores)]\n",
        "\n",
        "print(\"Best threshold:\", best_threshold)\n",
        "f1_score(y_test, y_pred, average='macro')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SUMSQ-2Ywkoz",
        "outputId": "31db6902-36a3-4071-f373-e3c1ff6848e6"
      },
      "id": "SUMSQ-2Ywkoz",
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best threshold: 0.1\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6265405330846991"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import joblib"
      ],
      "metadata": {
        "id": "RvBQh5pBf3At"
      },
      "id": "RvBQh5pBf3At",
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "artifacts_dict = {\n",
        "    \"model\": trainedModel,\n",
        "    \"train_df\": train_df,\n",
        "    \"threshold\": 0.63\n",
        "}\n",
        "artifacts_dict_file = open(\"artifacts_dict_file.pkl\", \"wb\")\n",
        "pickle.dump(obj=artifacts_dict, file=artifacts_dict_file)\n",
        "\n",
        "artifacts_dict_file.close() "
      ],
      "metadata": {
        "id": "LYJu1cXVa7_v"
      },
      "id": "LYJu1cXVa7_v",
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "b556b0fe",
      "metadata": {
        "id": "b556b0fe"
      },
      "source": [
        "## Stop Here. Create new file/notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21ded0cd",
      "metadata": {
        "id": "21ded0cd"
      },
      "source": [
        "## =============================================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "213f71c6",
      "metadata": {
        "id": "213f71c6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "a0d2e830",
      "metadata": {
        "id": "a0d2e830"
      },
      "source": [
        "## Model Scoring\n",
        "\n",
        "Write function that will load artifacts from above, transform and score on a new dataset.\n",
        "Your function should return Python list of labels. For example: [0,1,0,1,1,0,0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae1929ab",
      "metadata": {
        "id": "ae1929ab"
      },
      "outputs": [],
      "source": [
        "def project_1_scoring(data):\n",
        "    \"\"\"\n",
        "    Function to score input dataset.\n",
        "    \n",
        "    Input: dataset in Pandas DataFrame format\n",
        "    Output: Python list of labels in the same order as input records\n",
        "    \n",
        "    Flow:\n",
        "        - Load artifacts\n",
        "        - Transform dataset\n",
        "        - Score dataset\n",
        "        - Return pandas DF with following columns:\n",
        "            - index\n",
        "            - label\n",
        "            - probability_0\n",
        "            - probability_1\n",
        "    \"\"\"\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16157e5c",
      "metadata": {
        "id": "16157e5c"
      },
      "source": [
        "### Example of Scoring function\n",
        "\n",
        "Don't copy the code as is. It is provided as an example only. \n",
        "- Function `train_model` - you need to focus on model and artifacts saving:\n",
        "    ```\n",
        "    pickle.dump(obj=artifacts_dict, file=artifacts_dict_file)\n",
        "    ```\n",
        "- Function `project_1_scoring` - you should have similar function with name `project_1_scoring`. The function will:\n",
        "    - Get Pandas dataframe as parameter\n",
        "    - Will load model and all needed encoders\n",
        "    - Will perform needed manipulations on the input Pandas DF - in the exact same format as input file for the project, minus MIS_Status feature\n",
        "    - Return Pandas DataFrame\n",
        "        - record index\n",
        "        - predicted class for threshold maximizing F1\n",
        "        - probability for class 0 (PIF)\n",
        "        - probability for class 1 (CHGOFF)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b37c9dd",
      "metadata": {
        "id": "2b37c9dd"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Don't copy of use the cell code in any way!!!\n",
        "The code is provided as an example of generating artifacts for scoring function\n",
        "Your scoring function code should not have model training part!!!!\n",
        "\"\"\"\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "def train_model(data):\n",
        "    \"\"\"\n",
        "    Train sample model and save artifacts\n",
        "    \"\"\"\n",
        "    from sklearn.preprocessing import OneHotEncoder\n",
        "    from copy import deepcopy\n",
        "    from sklearn.linear_model import LogisticRegression\n",
        "    import pickle\n",
        "    from sklearn.impute import SimpleImputer\n",
        "    \n",
        "    target_col = \"Survived\"\n",
        "    cols_to_drop = ['Name', 'Ticket', 'Cabin','SibSp', 'Parch', 'Sex','Embarked','PassengerId','Survived']\n",
        "    y = data[target_col]\n",
        "    X = data.drop(columns=[target_col])\n",
        "    \n",
        "    # Impute Embarked\n",
        "    X['Embarked'].replace(np.NaN, 'S',inplace = True)\n",
        "    \n",
        "    # Create new feature\n",
        "    X['FamilySize'] = X['SibSp'] + X['Parch']\n",
        "    \n",
        "    # Mean impute Age\n",
        "    imp_age_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
        "    imp_age_mean.fit(X[['Age']])\n",
        "    X['Age'] = imp_age_mean.transform(X[['Age']])\n",
        "\n",
        "\n",
        "    ohe_orig_columns = [\"Embarked\",\"Sex\"]\n",
        "    cat_encoders = {}\n",
        "    for col in ohe_orig_columns:\n",
        "        enc = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
        "        enc.fit(X[[col]])\n",
        "        result = enc.transform(X[[col]])\n",
        "        ohe_columns = [col+\"_\"+str(x) for x in enc.categories_[0]]\n",
        "        result_train = pd.DataFrame(result, columns=ohe_columns)\n",
        "        X= pd.concat([X, result_train], axis=1)\n",
        "        cat_encoders[col] = [deepcopy(enc),\"ohe\"]\n",
        "        \n",
        "    clf = LogisticRegression(max_iter=1000, random_state=0)\n",
        "    \n",
        "    columns_to_train = [x for x in X.columns if x not in cols_to_drop]\n",
        "    clf.fit(X[columns_to_train], y)\n",
        "    \n",
        "    # Todo: Add code to calculate optimal threshold. Replace 0.5 !!!!!\n",
        "    threshold = 0.5\n",
        "    # End Todo\n",
        "    \n",
        "    artifacts_dict = {\n",
        "        \"model\": clf,\n",
        "        \"cat_encoders\": cat_encoders,\n",
        "        \"imp_age_mean\": imp_age_mean,\n",
        "        \"ohe_columns\": ohe_orig_columns,\n",
        "        \"columns_to_train\":columns_to_train,\n",
        "        \"threshold\": threshold\n",
        "    }\n",
        "    artifacts_dict_file = open(\"./artifacts/artifacts_dict_file.pkl\", \"wb\")\n",
        "    pickle.dump(obj=artifacts_dict, file=artifacts_dict_file)\n",
        "    \n",
        "    artifacts_dict_file.close()    \n",
        "    return clf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "719dcec9",
      "metadata": {
        "id": "719dcec9",
        "outputId": "93c2dcbc-dc7d-484f-9cac-f3ee7757ca51"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=1000, random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000, random_state=0)</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "LogisticRegression(max_iter=1000, random_state=0)"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_train = pd.read_csv('titanic.csv')\n",
        "train_model(df_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa88cb35",
      "metadata": {
        "id": "aa88cb35"
      },
      "source": [
        "### Example scoring function\n",
        "\n",
        "This is example only. Don't copy code as is!!!   \n",
        "You must place scoring function in a separate Python file or Jupyter notebook.   \n",
        "\n",
        "**Don't place function in the same notebook as rest of the code**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46d3d7f8",
      "metadata": {
        "id": "46d3d7f8"
      },
      "outputs": [],
      "source": [
        "def project_1_scoring(data):\n",
        "    \"\"\"\n",
        "    Function to score input dataset.\n",
        "    \n",
        "    Input: dataset in Pandas DataFrame format\n",
        "    Output: Python list of labels in the same order as input records\n",
        "    \n",
        "    Flow:\n",
        "        - Load artifacts\n",
        "        - Transform dataset\n",
        "        - Score dataset\n",
        "        - Return labels\n",
        "    \n",
        "    \"\"\"\n",
        "    from sklearn.preprocessing import OneHotEncoder\n",
        "    from copy import deepcopy\n",
        "    from sklearn.linear_model import LogisticRegression\n",
        "    import pickle\n",
        "    \n",
        "    X = data.copy()\n",
        "    \n",
        "    '''Load Artifacts'''\n",
        "    artifacts_dict_file = open(\"./artifacts/artifacts_dict_file.pkl\", \"rb\")\n",
        "    artifacts_dict = pickle.load(file=artifacts_dict_file)\n",
        "    artifacts_dict_file.close()\n",
        "    \n",
        "    clf = artifacts_dict[\"model\"]\n",
        "    cat_encoders = artifacts_dict[\"cat_encoders\"]\n",
        "    imp_age_mean = artifacts_dict[\"imp_age_mean\"]\n",
        "    ohe_columns = artifacts_dict[\"ohe_columns\"]\n",
        "    columns_to_score = artifacts_dict[\"columns_to_train\"]\n",
        "    threshold = artifacts_dict[\"threshold\"]\n",
        "    \n",
        "    # Impute Embarked\n",
        "    X['Embarked'].replace(np.NaN, 'S',inplace = True)\n",
        "    \n",
        "    # Create new feature\n",
        "    X['FamilySize'] = X['SibSp'] + X['Parch']\n",
        "    \n",
        "    # Mean impute Age\n",
        "    X['Age'] = imp_age_mean.transform(X[['Age']])\n",
        "    \n",
        "    '''Encode categorical columns'''\n",
        "    for col in ohe_columns:\n",
        "        enc = cat_encoders[col][0]\n",
        "        result = enc.transform(X[[col]])\n",
        "        ohe_columns = [col+\"_\"+str(x) for x in enc.categories_[0]]\n",
        "        result_train = pd.DataFrame(result, columns=ohe_columns)\n",
        "        X = pd.concat([X, result_train], axis=1)\n",
        "        \n",
        "    y_pred_proba = clf.predict_proba(X[columns_to_score])\n",
        "    y_pred = (y_pred_proba[:,0] < threshold).astype(np.int16)\n",
        "    d = {\"index\":data[\"PassengerId\"],\n",
        "         \"label\":y_pred,\n",
        "         \"probability_0\":y_pred_proba[:,0],\n",
        "         \"probability_1\":y_pred_proba[:,1]}\n",
        "    \n",
        "    return pd.DataFrame(d)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd9dbc26",
      "metadata": {
        "id": "fd9dbc26",
        "outputId": "91e6d2f5-1502-475b-af52-2837566f52df"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>label</th>\n",
              "      <th>probability_0</th>\n",
              "      <th>probability_1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.901298</td>\n",
              "      <td>0.098702</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0.071879</td>\n",
              "      <td>0.928121</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0.367665</td>\n",
              "      <td>0.632335</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0.098564</td>\n",
              "      <td>0.901436</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0.923460</td>\n",
              "      <td>0.076540</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   index  label  probability_0  probability_1\n",
              "0      1      0       0.901298       0.098702\n",
              "1      2      1       0.071879       0.928121\n",
              "2      3      1       0.367665       0.632335\n",
              "3      4      1       0.098564       0.901436\n",
              "4      5      0       0.923460       0.076540"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "project_1_scoring(df_train).head()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}